<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AR Real-time Segmentation (WebGLTexture Direct Input)</title>
  
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
        "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/"
      }
    }
  </script>
  <style>
    body { margin: 0; overflow: hidden; }
    #info, #glInfo {
      position: absolute;
      font-family: monospace;
      font-size: 12px;
      max-width: 90vw;
      white-space: pre-wrap;
      padding: 6px;
      background: rgba(0,0,0,0.6);
      color: white;
      z-index: 20;
    }
    #info { bottom: 10px; left: 10px; }
    #glInfo { top: 10px; right: 10px; color: lime; }
    #startBtn {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 30;
      padding: 10px;
      background: rgba(0,0,0,0.7);
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <button id="startBtn">Start AR Segmentation</button>
  <div id="info">Status: idle</div>
  <div id="glInfo"></div>
  <script type="module">
    import * as THREE from 'three';
    import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
    import { EffectComposer } from 'three/addons/postprocessing/EffectComposer.js';
    import { RenderPass } from 'three/addons/postprocessing/RenderPass.js';
    // Global state variables. They are initialized only after the DOM is
    // fully ready to avoid accessing elements before they exist.
    const modelUrl = 'https://vizbl-1.s3.eu-central-1.amazonaws.com/native/b617533f-d965-4782-8d22-c0574541b809.glb?version=1';
    const gltfLoader = new GLTFLoader();
    let gltfModel;
    let reticle, hitTestSource;

      let model, gl, session, glBinding;
    let processingLock, fpsHistory;
    let applyMask, renderer3D, scene3D, camera3D;

    // ---- GPU utilities ----
    class GlTextureImpl {
      constructor(gl, texture, width, height) {
        this.gl_ = gl;
        this.texture_ = texture;
        this.width = width;
        this.height = height;
      }
      bindTexture() {
        this.gl_.bindTexture(this.gl_.TEXTURE_2D, this.texture_);
      }
    }

    class GlTextureFramebuffer extends GlTextureImpl {
      constructor(gl, framebuffer, texture, width, height) {
        super(gl, texture, width, height);
        this.framebuffer_ = framebuffer;
      }
      bindFramebuffer() {
        this.gl_.bindFramebuffer(this.gl_.FRAMEBUFFER, this.framebuffer_);
        this.gl_.viewport(0, 0, this.width, this.height);
      }
    }

    class MaskStep {
      constructor(renderer) {
        this.renderer = renderer;
        this.scene = new THREE.Scene();
        this.camera = new THREE.OrthographicCamera(-1, 1, 1, -1, 0, 1);

        this.uniforms = {
          frame: { value: new THREE.Texture() },
          mask: { value: new THREE.Texture() }
        };

        const material = new THREE.ShaderMaterial({
          uniforms: this.uniforms,
          vertexShader: `varying vec2 vUv; void main(){ vUv = uv; gl_Position = vec4(position, 1.0); }`,
          fragmentShader: `\n            precision mediump float;\n            uniform sampler2D frame;\n            uniform sampler2D mask;\n            varying vec2 vUv;\n            void main(){\n              vec2 coord = vec2(1.0 - vUv.x, vUv.y);\n              vec4 src = texture2D(frame, coord);\n              float prob = texture2D(mask, coord).r;\n              float m = step(0.5, prob);\n              vec4 purple = vec4(0.365, 0.247, 0.827, 1.0);\n              gl_FragColor = vec4(mix(src.rgb, purple.rgb, m), 1.0);\n            }`,
          depthWrite: false,
          depthTest: false,
          transparent: true
        });

        const quad = new THREE.Mesh(new THREE.PlaneGeometry(2, 2), material);
        quad.renderOrder = 999;
        this.scene.add(quad);

        this.renderTarget = new THREE.WebGLRenderTarget(1, 1);
        this.composer = new EffectComposer(this.renderer, this.renderTarget);
        this.composer.addPass(new RenderPass(this.scene, this.camera));
      }

      process(frame, mask) {
        if (this.renderTarget.width !== frame.width || this.renderTarget.height !== frame.height) {
          this.renderTarget.setSize(frame.width, frame.height);
        }

        const props = this.renderer.properties;
        props.get(this.uniforms.frame.value).__webglTexture = frame.texture_;
        props.get(this.uniforms.mask.value).__webglTexture = mask.texture_;
        this.uniforms.frame.value.needsUpdate = false;
        this.uniforms.mask.value.needsUpdate = false;

        this.composer.render();
        this.renderer.setRenderTarget(null);

        const gl = this.renderer.getContext();
        const fb = props.get(this.renderTarget).__webglFramebuffer;
        const tex = props.get(this.renderTarget.texture).__webglTexture;
        return new GlTextureFramebuffer(gl, fb, tex, frame.width, frame.height);
      }
    }

    function createTextureFrameBuffer(gl, filterMode, width, height) {
      const framebuffer = gl.createFramebuffer();
      gl.bindFramebuffer(gl.FRAMEBUFFER, framebuffer);
      const texture = gl.createTexture();
      gl.bindTexture(gl.TEXTURE_2D, texture);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, filterMode);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, filterMode);
      gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, width, height, 0, gl.RGBA, gl.UNSIGNED_BYTE, null);
      gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);
      return new GlTextureFramebuffer(gl, framebuffer, texture, width, height);
    }

    function createTexture(gl, texture, width, height) {
      return new GlTextureImpl(gl, texture, width, height);
    }
    // ---- End GPU utilities ----

    function log(msg) {
      console.error(msg);
      document.getElementById('info').textContent = "Status:\n" + msg;
    }

    function logGLInfo(msg) {
      document.getElementById('glInfo').textContent = msg;
    }

    // Convert the AR camera WebGL texture directly into a 224x224 tensor
    // without syncing through the CPU. The AR camera provides an RGBA texture,
    // so we select only the RGB channels. The texture's pixels are already
    // normalized floats in [0,1], so no additional scaling is required.
    function preprocessTexture(tex, width, height) {
      return tf.tidy(() => {
        const rgb = tf.tensor(
          { texture: tex, width, height, channels: 'RGB' },
          [height, width, 3],
          'float32'
        );
        return tf.image.resizeBilinear(rgb, [224, 224])
          .expandDims();
      });
    }

    async function loadModel() {
      try {
        await tf.setBackend('webgl');
        await tf.ready();
        const backend = tf.backend();
        gl = backend?.getGPGPUContext?.().gl || backend?.gpgpu?.gl || backend?.gl;
        if (!gl) {
          throw new Error('Failed to obtain WebGL context');
        }
        if (typeof gl.makeXRCompatible === 'function') {
          try {
            if (gl.canvas && typeof gl.canvas.addEventListener === 'function') {
              await gl.makeXRCompatible();
            } else {
              log('Skipping XR compatibility: canvas lacks addEventListener');
            }
          } catch (err) {
            log('XR compatibility error: ' + err.message);
          }
        }
        log('Using WebGL backend');
        model = await tf.loadLayersModel('jsModel/model.json');
        log("Model loaded");
      } catch (e) {
        log('Load error: ' + e.message);
        throw e;
      }
    }

    async function startAR() {
      try {
        log("Starting AR...");

        await loadModel();

        session = await navigator.xr.requestSession('immersive-ar', {
          requiredFeatures: ['camera-access', 'dom-overlay', 'hit-test'],
          domOverlay: { root: document.body }
        });

        session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });
        const referenceSpace = await session.requestReferenceSpace('local');
        const viewerSpace = await session.requestReferenceSpace('viewer');
        hitTestSource = await session.requestHitTestSource({ space: viewerSpace });
        glBinding = new XRWebGLBinding(session, gl);

        renderer3D = new THREE.WebGLRenderer({ context: gl, alpha: true });
        renderer3D.autoClear = false;
        scene3D = new THREE.Scene();
        const hemiLight = new THREE.HemisphereLight(0xffffff, 0xbbbbff, 1);
        scene3D.add(hemiLight);
        camera3D = new THREE.PerspectiveCamera();
        camera3D.matrixAutoUpdate = false;
        applyMask = new MaskStep(renderer3D);

        gltfLoader.load('https://immersive-web.github.io/webxr-samples/media/gltf/reticle/reticle.gltf', (gltf) => {
          reticle = gltf.scene;
          reticle.visible = false;
          scene3D.add(reticle);
        });

        // TensorFlow.js may provide an OffscreenCanvas as the WebGL backend's canvas.
        // OffscreenCanvas is not a DOM node and cannot be appended to the document,
        // so only append when the canvas is an HTMLCanvasElement.
        const renderCanvas = gl.canvas;
        if (renderCanvas instanceof HTMLCanvasElement) {
          renderCanvas.width = window.innerWidth;
          renderCanvas.height = window.innerHeight;
          document.body.appendChild(renderCanvas);
        }

        try {
          const gltf = await gltfLoader.loadAsync(modelUrl);
          gltfModel = gltf.scene;
          gltfModel.scale.set(0.5, 0.5, 0.5);
        } catch (e) {
          log('Model load error: ' + e.message);
        }

        session.addEventListener('select', () => {
          if (gltfModel && reticle) {
            const clone = gltfModel.clone();
            clone.position.copy(reticle.position);
            scene3D.add(clone);
          }
        });

        let lastFrameTime = performance.now();

        async function onFrame(time, frame) {
          session.requestAnimationFrame(onFrame);
          if (processingLock.busy) return;
          const preprocessStart = performance.now();

          try {
            const pose = frame.getViewerPose(referenceSpace);
            if (!pose) return;

            const view = pose.views[0];
            const tex = glBinding.getCameraImage(view.camera);
            const texWidth = view.camera.width;
            const texHeight = view.camera.height;

            if (!tex) {
              logGLInfo("Camera texture is null");
              return;
            }

            processingLock.busy = true;

            
            let inputTensor;
            try {
              inputTensor = preprocessTexture(tex, texWidth, texHeight);
            } catch (err) {
              log("TF Error: " + err.message);
              processingLock.busy = false;
              return;
            }
            const preprocessTime = performance.now() - preprocessStart;

              const inferenceStart = performance.now();
              const prediction = model.predict(inputTensor).squeeze();
              await tf.nextFrame();
              const inferenceTime = performance.now() - inferenceStart;
              const postStart = performance.now();

              const maskTensor = prediction.expandDims(-1);
              const resizedMask = tf.image.resizeBilinear(maskTensor, [texHeight, texWidth]);
              const maskData = resizedMask.dataToGPU({customTexShape: [texHeight, texWidth]});
              const frameTex = createTexture(gl, tex, texWidth, texHeight);
              const maskTex = createTexture(gl, maskData.texture, texWidth, texHeight);
              const result = applyMask.process(frameTex, maskTex);
              gl.bindFramebuffer(gl.DRAW_FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
              gl.bindFramebuffer(gl.READ_FRAMEBUFFER, result.framebuffer_);
              gl.blitFramebuffer(0, 0, texWidth, texHeight, 0, texHeight, texWidth, 0, gl.COLOR_BUFFER_BIT, gl.NEAREST);
              gl.bindFramebuffer(gl.READ_FRAMEBUFFER, null);

              renderer3D.state.reset();
              renderer3D.setRenderTarget(null);
              const viewport = session.renderState.baseLayer.getViewport(view);
              renderer3D.setViewport(viewport.x, viewport.y, viewport.width, viewport.height);
              camera3D.matrix.fromArray(view.transform.matrix);
              camera3D.projectionMatrix.fromArray(view.projectionMatrix);
              camera3D.matrixWorldNeedsUpdate = true;

              const hitTestResults = frame.getHitTestResults(hitTestSource);
              if (hitTestResults.length > 0 && reticle) {
                const hitPose = hitTestResults[0].getPose(referenceSpace);
                reticle.visible = true;
                reticle.position.set(
                  hitPose.transform.position.x,
                  hitPose.transform.position.y,
                  hitPose.transform.position.z
                );
                reticle.updateMatrixWorld(true);
              }

              renderer3D.render(scene3D, camera3D);

              const postTime = performance.now() - postStart;

            maskData.tensorRef.dispose();
            tf.dispose([inputTensor, prediction, maskTensor, resizedMask]);
            processingLock.busy = false;

            const now = performance.now();
            const frameTime = now - lastFrameTime;
            lastFrameTime = now;
            fpsHistory.push(1000 / frameTime);
            if (fpsHistory.length > 30) fpsHistory.shift();
            const avgFps = fpsHistory.reduce((a, b) => a + b, 0) / fpsHistory.length;
            document.getElementById('info').textContent =
              `Preprocess: ${preprocessTime.toFixed(1)} ms\n` +
              `Inference: ${inferenceTime.toFixed(1)} ms\n` +
              `Postprocess: ${postTime.toFixed(1)} ms\n` +
              `FPS: ${avgFps.toFixed(1)}`;
          } catch (err) {
            log("Frame error: " + err.message);
            processingLock.busy = false;
          }
        }

        session.requestAnimationFrame(onFrame);
        document.getElementById('startBtn').style.display = 'none';
      } catch (e) {
        log('Unexpected error: ' + e.message);
      }
    }

    // Initialize DOM-dependent variables and attach the start button handler
    // once the document is ready. This ensures nothing runs before the user
    // explicitly clicks the button.
    function init() {
      processingLock = { busy: false };
      fpsHistory = [];

      const btn = document.getElementById('startBtn');
      if (!btn || typeof btn.addEventListener !== 'function') {
        log('startBtn is invalid or not found');
        return;
      }

      // Attach handler only after confirming the button exists to avoid
      // "a.addEventListener is not a function" errors.
      btn.addEventListener('click', startAR, { once: true });
    }

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', init);
    } else {
      init();
    }
  </script>
</body>
</html>
