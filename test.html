<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AR Real-time Segmentation (WebGLTexture Direct Input)</title>
  
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    #info, #glInfo {
      position: absolute;
      font-family: monospace;
      font-size: 12px;
      max-width: 90vw;
      white-space: pre-wrap;
      padding: 6px;
      background: rgba(0,0,0,0.6);
      color: white;
      z-index: 20;
    }
    #info { bottom: 10px; left: 10px; }
    #glInfo { top: 10px; right: 10px; color: lime; }
    #startBtn {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 30;
      padding: 10px;
      background: rgba(0,0,0,0.7);
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <button id="startBtn">Start AR Segmentation</button>
  <div id="info">Status: idle</div>
  <div id="glInfo"></div>

  <script>
    // Global state variables. They are initialized only after the DOM is
    // fully ready to avoid accessing elements before they exist.
      let model, gl, session, glBinding;
    let processingLock, fpsHistory;
    let applyMask;

    // ---- GPU mask post-processing utilities (adapted from tfjs-examples/gpu-pipeline) ----
    const PASSTHROUGH_VERTEX_SHADER = `#version 300 es
precision highp float;
in vec4 position;
in vec4 input_tex_coord;
out vec2 tex_coord;
void main() {
  gl_Position = position;
  tex_coord = input_tex_coord.xy;
}`;

    const MASK_SHADER = `#version 300 es
precision mediump float;
uniform sampler2D frame;
uniform sampler2D mask;
in highp vec2 tex_coord;
out vec4 out_color;
void main() {
  vec2 coord = vec2(1.0 - tex_coord[0], tex_coord[1]);
  vec4 src_color = texture(frame, coord).rgba;
  // The mask texture is a single-channel (R) image where the alpha channel
  // defaults to 1.0. Sampling the alpha channel would therefore always yield
  // 1.0 and result in the raw camera feed without any overlay. Use the red
  // channel to read the actual mask probability values.
  float probability = 255.0;
  if (probability > 0.5) {
    out_color = vec4(src_color.rgb, 255.0);
  } else {
    vec4 purple = vec4(0.365, 0.247, 0.827, 1.0);
    out_color = 0.5 * purple + 0.5 * src_color;
  }
  
}`;

    class GlTextureImpl {
      constructor(gl, texture, width, height) {
        this.gl_ = gl;
        this.texture_ = texture;
        this.width = width;
        this.height = height;
      }
      bindTexture() {
        this.gl_.bindTexture(this.gl_.TEXTURE_2D, this.texture_);
      }
    }

    class GlTextureFramebuffer extends GlTextureImpl {
      constructor(gl, framebuffer, texture, width, height) {
        super(gl, texture, width, height);
        this.framebuffer_ = framebuffer;
      }
      bindFramebuffer() {
        this.gl_.bindFramebuffer(this.gl_.FRAMEBUFFER, this.framebuffer_);
        this.gl_.viewport(0, 0, this.width, this.height);
      }
    }

    class GlProgramImpl {
      constructor(gl, program) {
        this.gl_ = gl;
        this.program_ = program;
        this.cachedUniformLocations_ = new Map();
      }
      useProgram() {
        this.gl_.useProgram(this.program_);
      }
      getUniformLocation(symbol) {
        if (!this.cachedUniformLocations_.has(symbol)) {
          const loc = this.gl_.getUniformLocation(this.program_, symbol);
          this.cachedUniformLocations_.set(symbol, loc);
        }
        return this.cachedUniformLocations_.get(symbol);
      }
    }

    class FullscreenQuad {
      constructor(gl) {
        this.gl_ = gl;
        this.squareBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, this.squareBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([-1, -1, 1, -1, -1, 1, 1, 1]), gl.STATIC_DRAW);
        this.texBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, this.texBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([0,0,1,0,0,1,1,1]), gl.STATIC_DRAW);
      }
      draw() {
        const gl = this.gl_;
        gl.enableVertexAttribArray(0);
        gl.bindBuffer(gl.ARRAY_BUFFER, this.squareBuffer);
        gl.vertexAttribPointer(0, 2, gl.FLOAT, false, 0, 0);
        gl.enableVertexAttribArray(1);
        gl.bindBuffer(gl.ARRAY_BUFFER, this.texBuffer);
        gl.vertexAttribPointer(1, 2, gl.FLOAT, false, 0, 0);
        gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
      }
    }

    class GlShaderProcessor {
      constructor(gl, shader) {
        this.gl = gl;
        this.quad = new FullscreenQuad(gl);
        this.program = createProgram(gl, PASSTHROUGH_VERTEX_SHADER, shader);
      }
      startProcessFrame(width, height) {
        if (!this.frame || this.frame.width !== width || this.frame.height !== height) {
          this.frame = createTextureFrameBuffer(this.gl, this.gl.LINEAR, width, height);
        }
        this.gl.viewport(0, 0, width, height);
        this.program.useProgram();
      }
      bindTextures(textures) {
        let unit = 0;
        for (const [name, tex] of textures) {
          const loc = this.program.getUniformLocation(name);
          this.gl.activeTexture(this.gl.TEXTURE0 + unit);
          tex.bindTexture();
          this.gl.uniform1i(loc, unit);
          unit++;
        }
      }
      finalizeProcessFrame() {
        this.frame.bindFramebuffer();
        this.quad.draw();
        return this.frame;
      }
    }

    class MaskStep {
      constructor(gl) {
        this.proc = new GlShaderProcessor(gl, MASK_SHADER);
      }
      process(frame, mask) {
        this.proc.startProcessFrame(frame.width, frame.height);
        this.proc.bindTextures([['frame', frame], ['mask', mask]]);
        return this.proc.finalizeProcessFrame();
      }
    }

    function createTextureFrameBuffer(gl, filterMode, width, height) {
      const framebuffer = gl.createFramebuffer();
      gl.bindFramebuffer(gl.FRAMEBUFFER, framebuffer);
      const texture = gl.createTexture();
      gl.bindTexture(gl.TEXTURE_2D, texture);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, filterMode);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, filterMode);
      gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, width, height, 0, gl.RGBA, gl.UNSIGNED_BYTE, null);
      gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);
      return new GlTextureFramebuffer(gl, framebuffer, texture, width, height);
    }

    function createTexture(gl, texture, width, height) {
      return new GlTextureImpl(gl, texture, width, height);
    }

    function createProgram(gl, vertexSrc, fragmentSrc) {
      const vs = gl.createShader(gl.VERTEX_SHADER);
      gl.shaderSource(vs, vertexSrc);
      gl.compileShader(vs);
      const fs = gl.createShader(gl.FRAGMENT_SHADER);
      gl.shaderSource(fs, fragmentSrc);
      gl.compileShader(fs);
      const program = gl.createProgram();
      gl.attachShader(program, vs);
      gl.attachShader(program, fs);
      gl.bindAttribLocation(program, 0, 'position');
      gl.bindAttribLocation(program, 1, 'input_tex_coord');
      gl.linkProgram(program);
      return new GlProgramImpl(gl, program);
    }
    // ---- End GPU mask utilities ----

    function log(msg) {
      console.error(msg);
      document.getElementById('info').textContent = "Status:\n" + msg;
    }

    function logGLInfo(msg) {
      document.getElementById('glInfo').textContent = msg;
    }

    // Convert the AR camera WebGL texture directly into a 224x224 tensor
    // without syncing through the CPU. The AR camera provides an RGBA texture,
    // so we select only the RGB channels. The texture's pixels are already
    // normalized floats in [0,1], so no additional scaling is required.
    function preprocessTexture(tex, width, height) {
      return tf.tidy(() => {
        const rgb = tf.tensor(
          { texture: tex, width, height, channels: 'RGB' },
          [height, width, 3],
          'float32'
        );
        return tf.image.resizeBilinear(rgb, [224, 224])
          .expandDims();
      });
    }

    async function loadModel() {
      try {
        await tf.setBackend('webgl');
        await tf.ready();
        const backend = tf.backend();
        gl = backend?.getGPGPUContext?.().gl || backend?.gpgpu?.gl || backend?.gl;
        if (!gl) {
          throw new Error('Failed to obtain WebGL context');
        }
        if (typeof gl.makeXRCompatible === 'function') {
          try {
            if (gl.canvas && typeof gl.canvas.addEventListener === 'function') {
              await gl.makeXRCompatible();
            } else {
              log('Skipping XR compatibility: canvas lacks addEventListener');
            }
          } catch (err) {
            log('XR compatibility error: ' + err.message);
          }
        }
        log('Using WebGL backend');
        applyMask = new MaskStep(gl);
        model = await tf.loadLayersModel('jsModel/model.json');
        log("Model loaded");
      } catch (e) {
        log('Load error: ' + e.message);
        throw e;
      }
    }

    async function startAR() {
      try {
        log("Starting AR...");

        await loadModel();

        // TensorFlow.js may provide an OffscreenCanvas as the WebGL backend's canvas.
        // OffscreenCanvas is not a DOM node and cannot be appended to the document,
        // so only append when the canvas is an HTMLCanvasElement.
        const renderCanvas = gl.canvas;
        if (renderCanvas instanceof HTMLCanvasElement) {
          renderCanvas.width = window.innerWidth;
          renderCanvas.height = window.innerHeight;
          document.body.appendChild(renderCanvas);
        }

        session = await navigator.xr.requestSession('immersive-ar', {
          requiredFeatures: ['camera-access', 'dom-overlay'],
          domOverlay: { root: document.body }
        });

        session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });
        const referenceSpace = await session.requestReferenceSpace('local');
        glBinding = new XRWebGLBinding(session, gl);

        let lastFrameTime = performance.now();

        async function onFrame(time, frame) {
          session.requestAnimationFrame(onFrame);
          if (processingLock.busy) return;
          const preprocessStart = performance.now();

          try {
            const pose = frame.getViewerPose(referenceSpace);
            if (!pose) return;

            const view = pose.views[0];
            const tex = glBinding.getCameraImage(view.camera);
            const texWidth = view.camera.width;
            const texHeight = view.camera.height;

            if (!tex) {
              logGLInfo("Camera texture is null");
              return;
            }

            processingLock.busy = true;

            
            let inputTensor;
            try {
              inputTensor = preprocessTexture(tex, texWidth, texHeight);
            } catch (err) {
              log("TF Error: " + err.message);
              processingLock.busy = false;
              return;
            }
            const preprocessTime = performance.now() - preprocessStart;

              const inferenceStart = performance.now();
              const prediction = model.predict(inputTensor).squeeze();
              await tf.nextFrame();
              const inferenceTime = performance.now() - inferenceStart;
              const postStart = performance.now();

              const maskTensor = prediction.expandDims(-1);
              const resizedMask = tf.image.resizeBilinear(maskTensor, [texHeight, texWidth]);
              const maskData = resizedMask.dataToGPU({customTexShape: [texHeight, texWidth]});
              const frameTex = createTexture(gl, tex, texWidth, texHeight);
              const maskTex = createTexture(gl, maskData.texture, texWidth, texHeight);
              const result = applyMask.process(frameTex, maskTex);
              gl.bindFramebuffer(gl.DRAW_FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
              gl.bindFramebuffer(gl.READ_FRAMEBUFFER, result.framebuffer_);
              gl.blitFramebuffer(0, 0, texWidth, texHeight, 0, texHeight, texWidth, 0, gl.COLOR_BUFFER_BIT, gl.NEAREST);
              const postTime = performance.now() - postStart;

            maskData.tensorRef.dispose();
            tf.dispose([inputTensor, prediction, maskTensor, resizedMask]);
            processingLock.busy = false;

            const now = performance.now();
            const frameTime = now - lastFrameTime;
            lastFrameTime = now;
            fpsHistory.push(1000 / frameTime);
            if (fpsHistory.length > 30) fpsHistory.shift();
            const avgFps = fpsHistory.reduce((a, b) => a + b, 0) / fpsHistory.length;
            document.getElementById('info').textContent =
              `Preprocess: ${preprocessTime.toFixed(1)} ms\n` +
              `Inference: ${inferenceTime.toFixed(1)} ms\n` +
              `Postprocess: ${postTime.toFixed(1)} ms\n` +
              `FPS: ${avgFps.toFixed(1)}`;
          } catch (err) {
            log("Frame error: " + err.message);
            processingLock.busy = false;
          }
        }

        session.requestAnimationFrame(onFrame);
        document.getElementById('startBtn').style.display = 'none';
      } catch (e) {
        log('Unexpected error: ' + e.message);
      }
    }

    // Initialize DOM-dependent variables and attach the start button handler
    // once the document is ready. This ensures nothing runs before the user
    // explicitly clicks the button.
    function init() {
      processingLock = { busy: false };
      fpsHistory = [];

      const btn = document.getElementById('startBtn');
      if (!btn || typeof btn.addEventListener !== 'function') {
        log('startBtn is invalid or not found');
        return;
      }

      // Attach handler only after confirming the button exists to avoid
      // "a.addEventListener is not a function" errors.
      btn.addEventListener('click', startAR, { once: true });
    }

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', init);
    } else {
      init();
    }
  </script>
</body>
</html>
