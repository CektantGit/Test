<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AR Segmentation</title>
  <script src="https://unpkg.com/three@0.122.0/build/three.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.7.0/dist/tf.min.js"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    canvas { display: block; }
    #startARBtn, #captureButton {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 12;
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 10px;
      border: none;
      border-radius: 5px;
      cursor: pointer;
    }
    #captureButton { top: 60px; }
    #maskImage {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: 11;
    }
    #info {
      position: absolute;
      bottom: 10px;
      left: 10px;
      z-index: 20;
      background: black;
      color: lime;
      padding: 10px;
      font-family: monospace;
      font-size: 12px;
      white-space: pre-wrap;
      max-width: 100%;
    }
  </style>
</head>
<body>
  <button id="startARBtn">Start AR</button>
  <button id="captureButton">Capture Image</button>
  <img id="maskImage"/>
  <div id="info">Log:</div>

  <script>
    let model, session, glBinding, gl, canvas, renderer, scene, camera, refSpace, xrCam;
    let captureNext = false;

    async function loadModel() {
      model = await tf.loadLayersModel('jsModel/model.json');
      log('Model loaded.');
    }

    function log(msg) {
      const info = document.getElementById('info');
      info.textContent += "\n" + msg;
    }

    async function startAR() {
      canvas = document.createElement('canvas');
      document.body.appendChild(canvas);
      gl = canvas.getContext('webgl', { xrCompatible: true });

      session = await navigator.xr.requestSession('immersive-ar', {
        requiredFeatures: ['hit-test', 'camera-access', 'dom-overlay'],
        domOverlay: { root: document.body }
      });

      await loadModel();

      session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });
      glBinding = new XRWebGLBinding(session, gl);

      refSpace = await session.requestReferenceSpace('local');

      renderer = new THREE.WebGLRenderer({ canvas: canvas, context: gl });
      renderer.autoClear = false;

      camera = new THREE.PerspectiveCamera();
      scene = new THREE.Scene();

      session.requestAnimationFrame(onXRFrame);
    }

    async function onXRFrame(time, frame) {
      session.requestAnimationFrame(onXRFrame);
      const pose = frame.getViewerPose(refSpace);
      if (!pose) return;

      const view = pose.views[0];
      xrCam = view.camera;
      if (!xrCam) {
        log("No XR camera found.");
        return;
      }

      const viewport = session.renderState.baseLayer.getViewport(view);
      renderer.setSize(viewport.width, viewport.height);

      camera.matrix.fromArray(view.transform.matrix);
      camera.projectionMatrix.fromArray(view.projectionMatrix);
      camera.updateMatrixWorld(true);

      gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
      renderer.render(scene, camera);

      if (captureNext) {
        captureNext = false;
        try {
          await captureImage(view);
        } catch (e) {
          log("Capture failed: " + e.message);
        }
      }
    }

    async function captureImage(view) {
      log("Capturing...");
      const texture = glBinding.getCameraImage(view.camera);
      if (!texture) {
        log("No camera texture available.");
        return;
      }

      const width = view.camera.width;
      const height = view.camera.height;

      const fbo = gl.createFramebuffer();
      gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);

      const outTex = gl.createTexture();
      gl.bindTexture(gl.TEXTURE_2D, outTex);
      gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, width, height, 0, gl.RGBA, gl.UNSIGNED_BYTE, null);
      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);

      gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, outTex, 0);

      gl.bindTexture(gl.TEXTURE_2D, texture);
      gl.copyTexImage2D(gl.TEXTURE_2D, 0, gl.RGBA, 0, 0, width, height, 0);

      let readback = new Uint8Array(width * height * 4);
      gl.readPixels(0, 0, width, height, gl.RGBA, gl.UNSIGNED_BYTE, readback);

      const flipped = new Uint8ClampedArray(readback.length);
      for (let y = 0; y < height; y++) {
        for (let x = 0; x < width; x++) {
          const i = (y * width + x) * 4;
          const j = ((height - y - 1) * width + x) * 4;
          flipped.set(readback.slice(i, i + 4), j);
        }
      }

      const offCanvas = document.createElement('canvas');
      offCanvas.width = width;
      offCanvas.height = height;
      const ctx = offCanvas.getContext('2d');
      ctx.putImageData(new ImageData(flipped, width, height), 0, 0);

      const tfimg = tf.browser.fromPixels(offCanvas).div(255).resizeBilinear([224, 224]).expandDims();
      const pred = await model.predict(tfimg);

      const maskData = pred.dataSync();
      const maskCanvas = document.createElement('canvas');
      maskCanvas.width = 224;
      maskCanvas.height = 224;
      const maskCtx = maskCanvas.getContext('2d');
      const imgData = maskCtx.createImageData(224, 224);

      for (let i = 0; i < 224 * 224; i++) {
        const val = maskData[i] * 255;
        imgData.data.set([val, val, val, 255], i * 4);
      }

      maskCtx.putImageData(imgData, 0, 0);
      document.getElementById('maskImage').src = maskCanvas.toDataURL();
      log("Capture complete.");
    }

    document.getElementById('startARBtn').addEventListener('click', startAR);
    document.getElementById('captureButton').addEventListener('click', () => captureNext = true);
  </script>
</body>
</html>
