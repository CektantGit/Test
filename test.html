<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AR Segmentation</title>
  <script src="https://unpkg.com/three@0.122.0/build/three.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.7.0/dist/tf.min.js"></script>
  <style>
    body { margin: 0; overflow: hidden; background: black; color: white; font-family: monospace; }
    #captureButton { position: absolute; top: 10px; left: 10px; z-index: 10; padding: 10px; background: rgba(0,0,0,0.8); color: white; border: none; border-radius: 5px; }
    #maskImage { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: contain; z-index: 5; pointer-events: none; }
    #info { position: absolute; bottom: 10px; left: 10px; z-index: 20; white-space: pre-wrap; font-size: 12px; max-width: 90vw; }
  </style>
</head>
<body>
  <button id="captureButton">Capture Image</button>
  <img id="maskImage" />
  <div id="info">Log:</div>

  <script>
    let model, gl, renderer, scene, camera, session, glBinding, referenceSpace, canvas;
    let captureInProgress = false;

    async function loadModel() {
      try {
        model = await tf.loadLayersModel('jsModel/model.json');
        log('Model loaded.');
      } catch (e) {
        log('Model load error: ' + e);
      }
    }

    function log(msg) {
      document.getElementById('info').innerText += '\n' + msg;
    }

    async function initAR() {
      canvas = document.createElement('canvas');
      document.body.appendChild(canvas);
      gl = canvas.getContext('webgl', { xrCompatible: true });
      renderer = new THREE.WebGLRenderer({ canvas, context: gl });
      renderer.autoClear = false;

      session = await navigator.xr.requestSession('immersive-ar', {
        requiredFeatures: ['hit-test', 'dom-overlay', 'camera-access'],
        domOverlay: { root: document.body }
      });

      session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });
      glBinding = new XRWebGLBinding(session, gl);
      referenceSpace = await session.requestReferenceSpace('local');

      scene = new THREE.Scene();
      camera = new THREE.PerspectiveCamera();
      camera.matrixAutoUpdate = false;

      session.requestAnimationFrame(onXRFrame);
    }

    async function onXRFrame(time, frame) {
      session.requestAnimationFrame(onXRFrame);
      const pose = frame.getViewerPose(referenceSpace);
      if (!pose) return;

      const view = pose.views[0];
      const viewport = session.renderState.baseLayer.getViewport(view);
      renderer.setSize(viewport.width, viewport.height);

      camera.matrix.fromArray(view.transform.matrix);
      camera.projectionMatrix.fromArray(view.projectionMatrix);
      camera.updateMatrixWorld(true);

      gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
      renderer.render(scene, camera);

      if (captureInProgress) {
        captureInProgress = false;
        await captureAndPredict(frame, view);
      }
    }

    async function captureAndPredict(frame, view) {
      try {
        log('Start capture...');
        const cameraTexture = glBinding.getCameraImage(view.camera);

        const tex = gl.createTexture();
        gl.bindTexture(gl.TEXTURE_2D, tex);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);

        const width = view.camera.width;
        const height = view.camera.height;

        gl.copyTexImage2D(gl.TEXTURE_2D, 0, gl.RGBA, 0, 0, width, height, 0);

        const pixels = new Uint8Array(width * height * 4);
        gl.readPixels(0, 0, width, height, gl.RGBA, gl.UNSIGNED_BYTE, pixels);

        const flipped = new Uint8ClampedArray(pixels.length);
        for (let y = 0; y < height; y++) {
          for (let x = 0; x < width; x++) {
            const src = (y * width + x) * 4;
            const dst = ((height - y - 1) * width + x) * 4;
            flipped.set(pixels.slice(src, src + 4), dst);
          }
        }

        const tmpCanvas = document.createElement('canvas');
        tmpCanvas.width = width;
        tmpCanvas.height = height;
        const ctx = tmpCanvas.getContext('2d');
        ctx.putImageData(new ImageData(flipped, width, height), 0, 0);

        const imgTensor = tf.browser.fromPixels(tmpCanvas).div(255.0);
        const resized = tf.image.resizeBilinear(imgTensor, [224, 224]).expandDims();

        const pred = await model.predict(resized);
        const data = pred.dataSync();

        const mask = document.createElement('canvas');
        mask.width = 224;
        mask.height = 224;
        const mctx = mask.getContext('2d');
        const imgData = mctx.createImageData(224, 224);
        for (let i = 0; i < 224 * 224; i++) {
          const val = data[i] * 255;
          imgData.data[i * 4] = val;
          imgData.data[i * 4 + 1] = val;
          imgData.data[i * 4 + 2] = val;
          imgData.data[i * 4 + 3] = 255;
        }
        mctx.putImageData(imgData, 0, 0);
        document.getElementById('maskImage').src = mask.toDataURL();

        log('Segmentation done.');
      } catch (e) {
        log('Capture error: ' + e);
      }
    }

    document.getElementById('captureButton').addEventListener('click', () => {
      captureInProgress = true;
    });

    window.addEventListener('load', async () => {
      await loadModel();
      await initAR();
    });
  </script>
</body>
</html>
